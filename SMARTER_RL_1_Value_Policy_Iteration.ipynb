{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMARTER_RL_1-Value_Policy_Iteration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPCtgUTeUUJG/DvlxMU3ym7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tstoyanov/smarter-rl/blob/main/SMARTER_RL_1_Value_Policy_Iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4fPp_O1t95q"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw7a3Ph5uAOK"
      },
      "source": [
        "# Preliminaries\n",
        "\n",
        "This notebook lets you import a gym environment and set up an agent that acts within the environment. Your tasks is to then implement some of the classical RL algorithms: Value iteration and Policy iteration. Play attention to how you are going to evaluate your agents.\n",
        "\n",
        "First, we make sure that all dependencies are met"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qktWZzEdtxNN"
      },
      "source": [
        "!pip install gym > /dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KLURrYHycqD"
      },
      "source": [
        "# Testing the Gym environments\n",
        "\n",
        "Our next step is to import the gym package, create an environment, and make sure that we can use it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlAGn42vyoyN",
        "outputId": "c44f824d-585b-4cb8-b04d-95c164ee2b84"
      },
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#create a cliff-walker\n",
        "env = gym.make('CliffWalking-v0')\n",
        "\n",
        "#set the start state\n",
        "state = env.reset()\n",
        "#and take some random actions\n",
        "for i in range(5):\n",
        "  #render the environment\n",
        "  env.render()\n",
        "  #select a random action\n",
        "  action = env.action_space.sample()\n",
        "  #take a step and record next state, reward and termination\n",
        "  state, reward, done, _ = env.step(action)\n",
        "  print(\"Acted: {}\".format(action))\n",
        "  print(\"State: {}\".format(state))\n",
        "  print(\"Reward: {}\".format(reward))\n",
        "  if done:\n",
        "    #this environment only terminates once the goal is reached\n",
        "    print(\"Done.\")\n",
        "    break"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Acted: 3\n",
            "State: 36\n",
            "Reward: -1\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Acted: 0\n",
            "State: 24\n",
            "Reward: -1\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Acted: 2\n",
            "State: 36\n",
            "Reward: -1\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Acted: 3\n",
            "State: 36\n",
            "Reward: -1\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Acted: 1\n",
            "State: 36\n",
            "Reward: -100\n"
          ]
        }
      ]
    }
  ]
}